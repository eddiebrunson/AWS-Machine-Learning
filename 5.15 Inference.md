# AWS Machine Learning Foundations 

### Lesson 5: Machine Learning with AWS DeepComposer

### 15. Inference

___


Once this model is trained, the generator network alone can be run to generate new accompaniments for a given input melody. If you recall, the model took as input a single-track piano roll representing melody and a noise vector to help generate varied output.

The final process for music generation then is as follows:

* Transform single-track music input into piano roll format.
* Create a series of random numbers to represent the random noise vector.
* Pass these as input to our trained generator model, producing a series of output piano rolls. Each output piano roll represents some instrument in the composition.
* Transform the series of piano rolls back into a common music format (MIDI), assigning an instrument for each track.


Apply your learning in AWS DeepComposer
[Try generating a musical composition in the symphony genre](https://console.aws.amazon.com/deepcomposer/home?region=us-east-1#musicStudio)


To explore this process firsthand, try loading a model in the music studio, using a sample model if you have not trained your own. After selecting from a prepared list of input melodies or recording your own, you may choose “Generate a composition” to generate accompaniments.
### Explore Generative AI Further
* [Create compositions using sample models in music studio](https://console.aws.amazon.com/deepcomposer/home?region=us-east-1#musicStudio)
* [Inspect the training of existing sample models](https://console.aws.amazon.com/deepcomposer/home?region=us-east-1#modelList)
* [Train your own model within the AWS DeepComposer console](https://console.aws.amazon.com/deepcomposer/home?region=us-east-1#trainModel)
* [Build your own GAN model](https://github.com/aws-samples/aws-deepcomposer-samples)


